# Blockchain Node Infrastructure

[![CI](https://github.com/danlix2000/blockchain-node-infra/actions/workflows/ci.yml/badge.svg)](https://github.com/danlix2000/blockchain-node-infra/actions/workflows/ci.yml)
[![Terraform](https://img.shields.io/badge/Terraform-%3E%3D1.10-623CE4?logo=terraform)](https://www.terraform.io/)
[![AWS](https://img.shields.io/badge/AWS-EC2%20%7C%20VPC-FF9900?logo=amazonwebservices)](https://aws.amazon.com/)
[![Latitude](https://img.shields.io/badge/Latitude.sh-Bare%20Metal-4A90D9?logo=serverfault)](https://www.latitude.sh/)
[![Ansible](https://img.shields.io/badge/Ansible-2.16-EE0000?logo=ansible)](https://www.ansible.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

Production-ready blockchain node infrastructure and deployment automation. Provisions AWS and Latitude bare metal with Terraform, configures nodes with Ansible, and secures RPC access via HAProxy (Let's Encrypt TLS, API-key auth, Route 53 DNS). Supports Docker and binary deployments, with included block-lag monitoring and load-testing tools.
  
> **Starting with Ethereum - expanding to Avalanche, BNB, Arbitrum, Optimism, Polygon, Base, Polkadot, IOTA and more.**

Ansible inventory is derived directly from Terraform state using the `cloud.terraform.terraform_provider` inventory plugin and Terraform `ansible_host` resources. The inventory plugin configuration file is auto-generated by Terraform.

## Table of Contents

- [Architecture](#architecture)
- [Supported Configurations](#supported-configurations)
- [Repository Structure](#repository-structure)
- [Quick Start](#quick-start)
  - [Prerequisites](#prerequisites)
  - [1. Clone and Setup Environment](#1-clone-and-setup-environment)
  - [2. Setup Credentials](#2-setup-credentials)
  - [3. Configure Infrastructure](#3-configure-infrastructure)
  - [4. Provision Infrastructure](#4-provision-infrastructure)
  - [5. Setup HAProxy Secrets (Vault)](#5-setup-haproxy-secrets-vault)
  - [6. Configure and Deploy Services](#6-configure-and-deploy-services)
  - [Deployment Scenarios](#deployment-scenarios)
  - [Deploying Specific Nodes](#deploying-specific-nodes)
- [Ethereum Node Configurations](#ethereum-node-configurations)
  - [Client Selection](#client-selection)
  - [Reth + Lighthouse (Archive)](#reth--lighthouse-archive)
  - [Geth + Prysm (Full Node)](#geth--prysm-full-node)
  - [Erigon + Caplin (Binary Build)](#erigon--caplin-binary-build)
  - [Storage Sizing](#storage-sizing)
  - [Network Ports](#network-ports)
- [HAProxy (Reverse Proxy)](#haproxy-reverse-proxy)
  - [HAProxy Configuration](#haproxy-configuration)
  - [HAProxy Ports](#haproxy-ports)
  - [HAProxy Variables](#haproxy-variables)
- [Documentation](#documentation)
- [Common Commands](#common-commands)
- [Security](#security)
  - [AWS (Security Groups)](#aws-security-groups)
  - [Latitude BM (UFW Firewall)](#latitude-bm-ufw-firewall)
  - [HAProxy (TLS + API Key Auth)](#haproxy-tls--api-key-auth)
- [Roadmap](#roadmap)
- [License](#license)

## Architecture

```
┌─────────────────────────────────────────────────────────────────────────┐
│                              Deployment Flow                             │
└─────────────────────────────────────────────────────────────────────────┘

  ┌──────────────┐     ┌──────────────┐     ┌──────────────────────┐
  │  Terraform   │────▶│   Ansible    │────▶│  Docker / Binary     │
  │  (Provision) │     │  (Configure) │     │  (Runtime)           │
  └──────────────┘     └──────────────┘     └──────────────────────┘
         │                    │                    │
         ▼                    ▼                    ▼
   AWS: VPC, EC2,       OS hardening,        Docker: Reth+LH, Geth+Prysm
   EBS, EIP             system setup         Binary: Erigon+Caplin
   Latitude: Import
   pre-ordered BMs
```

## Supported Configurations

| Chain    | Network  | Clients                | Node Type | Platform  | Deploy    | Status     |
|----------|----------|------------------------|-----------|-----------|-----------|------------|
| Ethereum | Mainnet  | Reth + Lighthouse      | Archive   | AWS       | Docker    | ✅ Ready   |
| Ethereum | Mainnet  | Geth + Prysm           | Full      | AWS       | Docker    | ✅ Ready   |
| Ethereum | Mainnet  | Reth + Lighthouse      | Archive   | Latitude  | Docker    | ✅ Ready   |
| Ethereum | Mainnet  | Geth + Prysm           | Full      | Latitude  | Docker    | ✅ Ready   |
| Ethereum | Mainnet  | Erigon + Caplin        | Full      | Latitude  | Binary    | ✅ Ready   |
| Ethereum | Mainnet  | Erigon + Caplin        | Full      | AWS       | Binary    | ✅ Ready   |
| Ethereum | Mainnet  | Erigon + Caplin        | Archive   | Latitude  | Binary    | ✅ Ready   |
| Ethereum | Mainnet  | Erigon + Caplin        | Archive   | AWS       | Binary    | ✅ Ready   |
| Ethereum | Sepolia  | Erigon + Caplin        | Full      | Latitude  | Binary    | ✅ Ready   |
| Ethereum | Sepolia  | Erigon + Caplin        | Full      | AWS       | Binary    | ✅ Ready   |
| Ethereum | Sepolia  | Erigon + Caplin        | Archive   | Latitude  | Binary    | ✅ Ready   |
| Ethereum | Sepolia  | Erigon + Caplin        | Archive   | AWS       | Binary    | ✅ Ready   |
| Ethereum | Sepolia  | Geth + Prysm           | Full      | AWS       | Docker    | ✅ Ready   |
| Ethereum | Sepolia  | Geth + Prysm           | Full      | Latitude  | Docker    | ✅ Ready   |
| Ethereum | Sepolia  | Reth + Lighthouse      | Archive   | AWS       | Docker    | ✅ Ready   |
| Ethereum | Sepolia  | Reth + Lighthouse      | Archive   | Latitude  | Docker    | ✅ Ready   |

## Repository Structure

```
.
├── terraform/
│   ├── stacks/
│   │   ├── aws/ethereum/mainnet/       # AWS Ethereum Mainnet
│   │   ├── aws/ethereum/sepolia/       # AWS Ethereum Sepolia
│   │   ├── latitude/ethereum/mainnet/  # Latitude BM Ethereum Mainnet
│   │   └── latitude/ethereum/sepolia/  # Latitude BM Ethereum Sepolia
│   └── modules/
│       ├── compute/                    # AWS EC2 instances
│       ├── compute-latitude/           # Latitude bare metal servers
│       └── network/                    # AWS VPC/subnets
├── ansible/
│   ├── playbooks/
│   │   ├── ethereum.yml        # Ethereum deployment (Docker)
│   │   ├── erigon.yml          # Erigon deployment (binary build)
│   │   ├── raid.yml            # RAID-0 setup (bare metal)
│   │   └── site.yml            # Entry point (imports ethereum + erigon)
│   ├── inventory/
│   │   ├── ethereum_mainnet_aws_terraform_state.yml      # Auto-generated (AWS)
│   │   ├── ethereum_mainnet_latitude_terraform_state.yml  # Auto-generated (Latitude)
│   │   ├── ethereum_sepolia_*_terraform_state.yml         # Auto-generated (Sepolia)
│   │   └── group_vars/
│   │       ├── all/                  # Global defaults (certbot email, data mount)
│   │       │   └── main.yml
│   │       ├── ethereum/             # Docker node config (role=ethereum) + vault secrets
│   │       │   ├── main.yml         # Versions, images, ports, vault refs
│   │       │   └── vault.yml        # Encrypted (ansible-vault create)
│   │       ├── erigon/              # Erigon node config (role=erigon) + vault secrets
│   │       │   ├── main.yml         # Version, RPC, Caplin, vault refs
│   │       │   └── vault.yml        # Encrypted (ansible-vault create)
│   │       ├── node_archive/        # Archive node overrides (prune.mode, commitment-history)
│   │       │   └── main.yml
│   │       ├── network_mainnet/      # Mainnet network overrides
│   │       │   └── main.yml
│   │       ├── network_sepolia/      # Sepolia network overrides (chain, RPC tuning)
│   │       │   └── main.yml
│   │       └── platform_latitude/    # Latitude BM platform overrides
│   │           ├── main.yml         # SSH user, firewall, vault refs
│   │           └── vault.yml        # Encrypted AWS creds (BM only)
│   ├── roles/
│   │   ├── common/             # Shared (system setup, disk, optional Docker)
│   │   ├── raid/               # RAID-0 auto-discovery (bare metal)
│   │   ├── ethereum/           # Ethereum Docker clients (Reth+LH, Geth+Prysm)
│   │   ├── erigon/             # Erigon binary build (built-in Caplin CL)
│   │   └── haproxy/            # Reverse proxy (TLS, API key auth, WebSocket)
├── examples/
│   ├── docker/ethereum/
│   │   ├── reth-lighthouse/    # Reth+Lighthouse (full & archive profiles)
│   │   └── geth-prysm/         # Geth+Prysm full node
│   └── systemd/
│       └── erigon/             # Erigon systemd service (binary, no Docker)
├── tools/
│   ├── block-lag-monitor/      # RPC block lag & latency monitoring (EVM + Avalanche)
│   └── load-test/              # JSON-RPC load testing with Paradigm's flood
├── docs/
│   ├── ansible.md             # Ansible configuration guide
│   ├── ops.md                 # Operations guide
│   ├── terraform.md           # Terraform infrastructure guide
│   └── static-inventory.md    # Deploy without Terraform
├── images/                    # Screenshots for documentation
└── .github/workflows/
```

`examples/` contains standalone reference configs (Docker Compose and systemd) for testing and quick setups. Production deployments use Ansible.
`tools/` contains operational utilities. See [Block Lag Monitor](tools/block-lag-monitor/) for RPC block lag & latency monitoring and [Load Test](tools/load-test/) for JSON-RPC load testing with Paradigm's flood.

> **Naming Conventions:**
> - Terraform stacks: `terraform/stacks/{platform}/{chain}/{network}/`
> - Inventory files: `ansible/inventory/{chain}_{network}_{platform}_terraform_state.yml`

## Quick Start

### Prerequisites

- [HCP Terraform](https://app.terraform.io) account (free tier available)
- Terraform >= 1.10
- Python >= 3.10
- Docker (for local testing)
- **AWS:** AWS CLI configured with credentials
- **Latitude:** [Latitude.sh](https://latitude.sh) account + API token

### 1. Clone and Setup Environment

```bash
# Clone the repository
git clone https://github.com/YOUR_USERNAME/blockchain-infra.git
cd blockchain-infra

# Create Python virtual environment
python3 -m venv .venv
source .venv/bin/activate  # Linux/macOS
# .venv\Scripts\activate   # Windows

# Install Python dependencies (Ansible)
pip install -r requirements.txt

# Install Ansible Galaxy collections
ansible-galaxy collection install -r ansible/requirements.yml
```

### 2. Setup Credentials

```bash
# HCP Terraform authentication (required for all platforms)
export TF_TOKEN_app_terraform_io="your-api-token"
# Or: terraform login

# AWS credentials
export AWS_ACCESS_KEY_ID="AKIA..."
export AWS_SECRET_ACCESS_KEY="..."
export AWS_REGION="us-east-1"
# Or: aws configure

# Latitude.sh credentials (for bare metal deployments)
export LATITUDESH_AUTH_TOKEN="your-latitude-api-token"
```

> **Note:** This repo uses **local execution** mode - Terraform runs on your machine while TF Cloud stores state only. Set execution mode in TF Cloud: Workspace > Settings > General > Execution Mode > Local.

### 3. Configure Infrastructure

**AWS:**
```bash
cd terraform/stacks/aws/ethereum/mainnet
cp cloud.tf.example cloud.tf
cp terraform.tfvars.example terraform.tfvars
cp nodes.tfvars.example nodes.tfvars
# Edit terraform.tfvars and nodes.tfvars for your deployment
```

**Latitude.sh (Bare Metal):**
```bash
# 1. Order servers on Latitude.sh dashboard (select plan, site, OS - no RAID)
# 2. Rename hostname in console (e.g., chi-ethereum-mainnet-full)
# 3. Copy server IDs (sv_xxx) from dashboard

cd terraform/stacks/latitude/ethereum/mainnet
cp cloud.tf.example cloud.tf
cp terraform.tfvars.example terraform.tfvars
cp nodes.tfvars.example nodes.tfvars
# Edit terraform.tfvars (set project_id, ssh_key_ids)
# Edit nodes.tfvars (set latitude_id for each pre-ordered server)
```

### 4. Provision Infrastructure

```bash
# AWS Mainnet
cd terraform/stacks/aws/ethereum/mainnet
terraform init && terraform apply -var-file=nodes.tfvars
# -> generates ansible/inventory/ethereum_mainnet_aws_terraform_state.yml

# AWS Sepolia
cd terraform/stacks/aws/ethereum/sepolia
terraform init && terraform apply -var-file=nodes.tfvars
# -> generates ansible/inventory/ethereum_sepolia_aws_terraform_state.yml

# Latitude Mainnet (imports pre-ordered servers, separate workspace/state)
cd terraform/stacks/latitude/ethereum/mainnet
terraform init && terraform apply -var-file=nodes.tfvars
# -> generates ansible/inventory/ethereum_mainnet_latitude_terraform_state.yml

# Latitude Sepolia
cd terraform/stacks/latitude/ethereum/sepolia
terraform init && terraform apply -var-file=nodes.tfvars
# -> generates ansible/inventory/ethereum_sepolia_latitude_terraform_state.yml
```

### 5. Setup HAProxy Secrets (Vault)

> **Skip this step** if you do not need HAProxy (TLS + API key auth). Nodes will still deploy and sync without HAProxy. You can add HAProxy later by completing this step and re-running the playbook with `--tags haproxy`.

HAProxy requires encrypted vault files with API keys before deployment. Create one vault per role you deploy. For full details, see [HAProxy Configuration](#haproxy-configuration) or [Ansible Vault Setup](docs/ansible.md#ansible-vault-setup).

```bash
cd ansible

# Step 1: Create vault password file
echo "your-vault-password" > .vault_password
chmod 600 .vault_password

# Step 2: Create vault for Docker nodes (role = "ethereum")
uuidgen  # Generate a UUID4 API key, copy the output
ansible-vault create inventory/group_vars/ethereum/vault.yml
# -> Editor opens. Add:
#    vault_haproxy_api_key: "<paste-uuid4>"
#    vault_haproxy_stats_password: "<choose-password>"

# Step 3: Create vault for Erigon nodes (role = "erigon")
uuidgen  # Generate a DIFFERENT UUID4
ansible-vault create inventory/group_vars/erigon/vault.yml
# -> Same format, different API key

# Step 4 (Latitude BM only): Create vault for AWS credentials
# ansible-vault create inventory/group_vars/platform_latitude/vault.yml
# -> Add: vault_haproxy_certbot_aws_access_key, vault_haproxy_certbot_aws_secret_key
```

> Use a different API key per role group. `.vault_password` is in `.gitignore` - store the password securely.

### 6. Configure and Deploy Services

```bash
cd ansible

# Terraform Cloud token for inventory access
export TF_TOKEN_app_terraform_io="your-api-token"
export ANSIBLE_PRIVATE_KEY_FILE=~/.ssh/your-key.pem
```

Host key checking is enabled. Ensure hosts are present in `~/.ssh/known_hosts` before deployment.

Different node types use different Ansible playbooks. Match the playbook to the `role` in your `nodes.tfvars`:

| `role` in nodes.tfvars | Playbook | Clients |
|------------------------|----------|---------|
| `ethereum` | `playbooks/ethereum.yml` | Docker: Geth+Prysm, Reth+Lighthouse |
| `erigon` | `playbooks/erigon.yml` | Binary: Erigon+Caplin |

> **Key rule:** Run the correct playbook for each role. If your inventory has both `ethereum` and `erigon` nodes, you need two separate playbook runs.

### Deployment Scenarios

#### Scenario 1: Single role - all nodes same type

All nodes in the inventory use the same role (e.g., all Docker or all Erigon):

```bash
# All Docker nodes (Geth+Prysm, Reth+Lighthouse) - AWS Mainnet
ansible-playbook -i inventory/ethereum_mainnet_aws_terraform_state.yml playbooks/ethereum.yml

# All Erigon nodes (binary build) - AWS Mainnet
ansible-playbook -i inventory/ethereum_mainnet_aws_terraform_state.yml playbooks/erigon.yml
```

#### Scenario 2: Mixed roles - Ethereum + Erigon on the same platform

When your `nodes.tfvars` has nodes with different roles (e.g., a Geth+Prysm full node and an Erigon archive node), run each playbook separately. Use `--limit` to target the right nodes:

```bash
INV=inventory/ethereum_mainnet_aws_terraform_state.yml

# Step 1: Deploy the Ethereum full node (Geth+Prysm, Docker)
ansible-playbook -i $INV playbooks/ethereum.yml --limit node_full

# Step 2: Deploy the Erigon archive node (binary build)
ansible-playbook -i $INV playbooks/erigon.yml --limit node_archive
```

Without `--limit`, the playbook runs against all nodes in the inventory. The playbook itself targets its own role group, but using `--limit` is recommended to avoid unnecessary connection attempts to nodes that belong to a different role.

#### Scenario 3: Specific node only

Deploy or reconfigure a single node without touching others:

```bash
# Terraform: provision only the erigon-arch node
terraform apply -var-file=nodes.tfvars -target='module.compute["erigon-arch"]'

# Ansible: configure only that node
ansible-playbook -i inventory/ethereum_mainnet_aws_terraform_state.yml playbooks/erigon.yml --limit node_archive
```

#### Scenario 4: Latitude BM (RAID-0 + node deployment)

Latitude bare metal servers may have multiple data disks. Run RAID setup first, then the node playbook:

```bash
INV=inventory/ethereum_mainnet_latitude_terraform_state.yml

# Step 1: Verify connectivity
ansible -i $INV all -m ping

# Step 2: Setup RAID-0 (auto-discovers data disks, creates /dev/md127)
# Skip if BM has only 1 data disk - common role handles single disks
ansible-playbook -i $INV playbooks/raid.yml

# Step 3: Deploy nodes (choose the correct playbook for your role)
ansible-playbook -i $INV playbooks/ethereum.yml   # Docker nodes
ansible-playbook -i $INV playbooks/erigon.yml      # Erigon nodes
```

For mixed roles on Latitude BM, combine RAID + targeted playbooks:

```bash
INV=inventory/ethereum_mainnet_latitude_terraform_state.yml

ansible-playbook -i $INV playbooks/raid.yml
ansible-playbook -i $INV playbooks/ethereum.yml --limit node_full
ansible-playbook -i $INV playbooks/erigon.yml --limit node_archive
```

#### Scenario 5: Sepolia testnet

Same commands, different inventory file:

```bash
# AWS Sepolia - Docker nodes
ansible-playbook -i inventory/ethereum_sepolia_aws_terraform_state.yml playbooks/ethereum.yml

# AWS Sepolia - Erigon nodes
ansible-playbook -i inventory/ethereum_sepolia_aws_terraform_state.yml playbooks/erigon.yml

# Latitude BM Sepolia - RAID + Erigon
ansible-playbook -i inventory/ethereum_sepolia_latitude_terraform_state.yml playbooks/raid.yml
ansible-playbook -i inventory/ethereum_sepolia_latitude_terraform_state.yml playbooks/erigon.yml
```

#### Scenario 6: HAProxy only (update proxy without redeploying node)

```bash
# Erigon nodes - HAProxy only
ansible-playbook -i inventory/ethereum_mainnet_aws_terraform_state.yml playbooks/erigon.yml --tags haproxy

# Docker nodes - HAProxy only
ansible-playbook -i inventory/ethereum_mainnet_aws_terraform_state.yml playbooks/ethereum.yml --tags haproxy

# Deploy node without HAProxy
ansible-playbook -i inventory/ethereum_mainnet_aws_terraform_state.yml playbooks/erigon.yml --skip-tags haproxy
```

#### Scenario 7: Static inventory (no Terraform)

For manually provisioned servers without Terraform state (see [Static Inventory Guide](docs/static-inventory.md)):

```bash
ansible-playbook -i inventory/ethereum_sepolia_hosts.yml playbooks/erigon.yml
```

### Deploying Specific Nodes

**Terraform** - use `-target` to provision specific nodes:

```bash
terraform apply -var-file=nodes.tfvars -target='module.compute["full"]'
terraform apply -var-file=nodes.tfvars -target='module.compute["erigon-arch"]'
```

**Ansible** - use `--limit` to target specific node groups:

```bash
# Single node group
ansible-playbook -i inventory/ethereum_mainnet_aws_terraform_state.yml playbooks/ethereum.yml --limit node_full
ansible-playbook -i inventory/ethereum_mainnet_aws_terraform_state.yml playbooks/erigon.yml --limit node_archive

# Multiple node groups
ansible-playbook -i inventory/ethereum_mainnet_aws_terraform_state.yml playbooks/ethereum.yml --limit "node_full,node_archive"
```

## Ethereum Node Configurations

### Client Selection

| Variable | Options | Description |
|----------|---------|-------------|
| `role` | `ethereum`, `erigon` | Deployment type (Docker or binary) |
| `execution_client` | `reth`, `geth`, `erigon` | Execution layer client |
| `consensus_client` | `lighthouse`, `prysm`, `caplin` | Consensus layer client |
| `node_type` | `archive`, `full` | Node synchronization mode |

### Reth + Lighthouse (Archive)

Deploys as a **Lighthouse Beacon Archive** with full blob retention (`--prune-blobs=false`, `--supernode`). Reth runs in default mode (no `--full` flag) which stores all historical state without pruning.

**L2 Beacon Endpoint:** Both mainnet and Sepolia Lighthouse archive nodes expose the Beacon API on port 5052 (`--http-port=5052`). This provides an L1 beacon endpoint for L2 chains (Arbitrum, Optimism, Base, etc.) that require historical blob data access via the `/eth/v1/beacon/blob_sidecars/{block_id}` API.

**Checkpoint Sync:** Lighthouse supports fast sync from a finalized checkpoint. This is optional but recommended as it is substantially faster than syncing from genesis. Endpoint availability varies by network:
- **Mainnet:** `https://mainnet.checkpoint.sigp.io` (Sigma Prime, stable)
- **Sepolia:** `http://unstable.sepolia.beacon-api.nimbus.team` (Nimbus team, unstable/non-commercial). Standard Sepolia endpoints (`beaconstate.info`, `ethpandaops.io`, `chainsafe.io`) do not serve blobs, which Lighthouse v8.0.0+ requires for post-Deneb checkpoint sync.



| Resource | Minimum | Recommended |
|----------|---------|-------------|
| CPU      | 8 vCPU  | 16 vCPU (higher clock speed over core count) |
| Memory   | 32 GB   | 64 GB       |
| Storage  | 3 TB    | 5+ TB (TLC NVMe recommended) |

> [Reth system requirements](https://reth.rs/run/system-requirements/)

### Geth + Prysm (Full Node)


| Resource | Minimum | Recommended |
|----------|---------|-------------|
| CPU      | 4 vCPU  | 8 vCPU      |
| Memory   | 16 GB   | 32 GB       |
| Storage  | 1 TB    | 2 TB        |

> [Geth hardware requirements](https://geth.ethereum.org/docs/getting-started/hardware-requirements)

### Erigon + Caplin (Binary Build)

Erigon v3 with built-in Caplin consensus client. Deployed as a native binary (no Docker). Single process handles both execution and consensus layers.

- **Full** (default): Erigon's native mode. No `--prune.mode` flag needed.
- **Archive**: `--prune.mode=archive` + `--experimental.commitment-history`, applied via `group_vars/node_archive/` when `node_type = "archive"`.



| Resource | Full | Archive |
|----------|------|---------|
| CPU      | 8+ vCPU | 16+ vCPU |
| Memory   | 32+ GB | 64+ GB |
| Storage  | 2+ TB | 3+ TB (RAID-0 recommended) |

> [Erigon hardware requirements](https://docs.erigon.tech/get-started/hardware-requirements)

### Storage Sizing

| Configuration | Execution | Consensus | Total |
|---------------|-----------|-----------|-------|
| Reth + Lighthouse (archive) | ~3 TB | ~500 GB | 5 TB |
| Geth + Prysm (full) | ~1 TB | ~200 GB | 2 TB |
| Erigon + Caplin (full) | ~1.5 TB | built-in | 2 TB |
| Erigon + Caplin (archive) | ~2.5 TB | built-in | 3 TB |

### Network Ports

**Docker nodes (Reth+Lighthouse, Geth+Prysm):**

| Port  | Protocol | Client     | Purpose           |
|-------|----------|------------|-------------------|
| 30303 | TCP/UDP  | Reth/Geth  | Execution P2P     |
| 9000  | TCP/UDP  | Lighthouse | Consensus P2P     |
| 9001  | UDP      | Lighthouse | Consensus QUIC    |
| 13000 | TCP      | Prysm      | Consensus P2P TCP |
| 12000 | UDP      | Prysm      | Consensus P2P UDP |
| 8545  | TCP      | All        | HTTP RPC          |
| 8546  | TCP      | All        | WebSocket RPC     |
| 5052  | TCP      | Lighthouse | Beacon HTTP API (L1 beacon for L2 chains) |
| 3500  | TCP      | Prysm      | Beacon HTTP API   |

**Erigon nodes (binary build, UFW firewall):**

| Port  | Protocol | Component  | Purpose           | Expose   |
|-------|----------|------------|-------------------|----------|
| 53    | TCP/UDP  | System     | DNS               | Public   |
| 30303 | TCP/UDP  | Erigon     | eth/68 P2P        | Public   |
| 30304 | TCP/UDP  | Erigon     | eth/69 P2P        | Public   |
| 42069 | TCP/UDP  | Erigon     | Torrent/snap sync | Public   |
| 4000  | UDP      | Caplin     | CL discovery      | Public   |
| 4001  | TCP      | Caplin     | CL discovery      | Public   |
| 8545  | TCP      | Erigon     | HTTP RPC          | Localhost |
| 8546  | TCP      | Erigon     | WebSocket RPC     | Localhost |
| 8551  | TCP      | Erigon     | Engine API (JWT)  | VPN only |
| 9090  | TCP      | Erigon     | gRPC server       | VPN only |

> **UFW Firewall:** Erigon bare metal nodes use UFW. Public ports are open to all. RPC ports (8545, 8546) bind to `127.0.0.1` only - external access is via HAProxy (443). Engine API and gRPC are accessible from VPN IPs configured via `erigon_firewall_allowed_ips` in group_vars (`group_vars/erigon/main.yml` for mainnet, `group_vars/network_sepolia/main.yml` for Sepolia).

## HAProxy (Reverse Proxy)

HAProxy provides secure external RPC access with TLS termination, UUID4 API key authentication, and WebSocket support. Runs on the same node as the blockchain client, proxying to localhost backends.

**Features:**
- Let's Encrypt TLS via Certbot + Route 53 DNS-01 challenge (auto-renewal)
- API key authentication via `X-API-Key` header or `?apikey=` URL parameter (file-based lookup)
- WebSocket upgrade detection with extended timeouts (1h tunnel)
- HTTP to HTTPS redirect
- Optional HAProxy stats page and Prometheus metrics exporter

### HAProxy Configuration

> **Prerequisites:** Domain with [AWS Route 53](https://aws.amazon.com/route53/) hosted zone (required for Let's Encrypt DNS-01 challenge). Complete these steps **before** running the Ansible playbooks in [Step 6](#6-configure-and-deploy-services). For the full vault reference, see [Ansible Vault Setup](docs/ansible.md#ansible-vault-setup).

**Step 1: Create vault password file:**

```bash
cd ansible
echo "your-vault-password" > .vault_password
chmod 600 .vault_password
```

> `.vault_password` is in `.gitignore`. Store the password securely and share with team members out-of-band.

**Step 2: Create per-role secrets vault** (API key + stats password):

Each role group gets its own vault so different node types use different API keys. Create one vault per role you deploy:

```bash
uuidgen  # Generate a UUID4 API key, copy the output

# For Docker nodes (Reth+Lighthouse, Geth+Prysm):
ansible-vault create inventory/group_vars/ethereum/vault.yml

# For Erigon nodes (Erigon+Caplin):
ansible-vault create inventory/group_vars/erigon/vault.yml
```

Your editor opens for each file. Add the following variables, save, and close:

```yaml
---
vault_haproxy_api_key: "<paste-uuid4-from-uuidgen>"
vault_haproxy_stats_password: "<choose-a-stats-password>"
```

> Use a different `uuidgen` output for each role group. This isolates API keys so compromising one key does not expose other node types.

**Step 3: Create Latitude BM vault** (skip for AWS-only deployments):

Create an IAM user with Route 53 permissions (`route53:GetChange`, `route53:ListHostedZones`, `route53:ListHostedZonesByName`, `route53:ChangeResourceRecordSets`), then:

```bash
ansible-vault create inventory/group_vars/platform_latitude/vault.yml
```

Your editor opens. Add the following variables, save, and close:

```yaml
---
vault_haproxy_certbot_aws_access_key: "<iam-access-key>"
vault_haproxy_certbot_aws_secret_key: "<iam-secret-key>"
```

> AWS EC2 nodes use IAM instance profiles for Certbot Route 53 - no AWS credentials needed. Only Latitude BM nodes require this vault file.

**Step 4: Set deployment config** (plaintext, not secrets):

Edit `inventory/group_vars/all/main.yml` and set the certbot email:

```yaml
haproxy_certbot_email: "admin@example.com"
```

> `haproxy_domain` is set per-node via Terraform: add `domain` in `nodes.tfvars` and `route53_zone_id` in `terraform.tfvars`. Terraform creates Route 53 A records and passes `haproxy_domain` as a host variable automatically. See [Terraform DNS](docs/terraform.md#dns-route-53-a-records).

**Deploy** (see [Step 6: Configure and Deploy Services](#6-configure-and-deploy-services) for full scenarios):

```bash
cd ansible

# Full deploy (node + HAProxy)
ansible-playbook -i inventory/ethereum_mainnet_aws_terraform_state.yml playbooks/erigon.yml

# HAProxy only (skip node rebuild)
ansible-playbook -i inventory/ethereum_mainnet_aws_terraform_state.yml playbooks/erigon.yml --tags haproxy

# Node only (skip HAProxy)
ansible-playbook -i inventory/ethereum_mainnet_aws_terraform_state.yml playbooks/erigon.yml --skip-tags haproxy
```

**Verify:**

API key can be passed via `X-API-Key` header or `?apikey=` URL parameter:

```bash
# Method 1: API key via header
curl -s https://rpc.eth.example.com/ \
  -H "X-API-Key: <your-uuid4-api-key>" \
  -X POST -H "Content-Type: application/json" \
  -d '{"jsonrpc":"2.0","method":"eth_syncing","params":[],"id":1}'

# Method 2: API key via URL parameter
curl -s -X POST \
  -H "Content-Type: application/json" \
  -d '{"jsonrpc":"2.0","method":"eth_syncing","params":[],"id":1}' \
  "https://rpc.eth.example.com/?apikey=<your-uuid4-api-key>"

# Test without API key (should return 403)
curl -s https://rpc.eth.example.com/

# Test WebSocket
wscat -c wss://rpc.eth.example.com/ -H "X-API-Key: <your-uuid4-api-key>"
```

`eth_syncing` returns sync progress (`currentBlock`, `highestBlock`) or `false` when fully synced. For continuous monitoring, use the [Block Lag Monitor](tools/block-lag-monitor/) to track block lag, latency, and estimate sync ETA across all your nodes.

> For vault management commands (edit, view, rekey), see [Ansible Vault Setup](docs/ansible.md#ansible-vault-setup).

### HAProxy Ports

| Port | Protocol | Purpose | Expose |
|------|----------|---------|--------|
| 80   | TCP      | HTTP (redirects to HTTPS) | Public |
| 443  | TCP      | HTTPS (TLS + API key auth) | Public |
| 8404 | TCP      | Stats page (localhost only) | Local |
| 8405 | TCP      | Prometheus metrics (optional) | Local |

### HAProxy Variables

All variables have sensible defaults in `roles/haproxy/defaults/main.yml`. Backend ports are auto-mapped from execution client ports via group_vars:

- **Erigon** (`group_vars/erigon/main.yml`): `haproxy_backend_http_port: "{{ erigon_http_port }}"` (8545)
- **Docker** (`group_vars/ethereum/main.yml`): `haproxy_backend_http_port: "{{ ports.execution_http }}"` (8545)

## Documentation

- [Terraform](docs/terraform.md) - Infrastructure documentation
- [Ansible](docs/ansible.md) - Configuration management guide
- [Operations](docs/ops.md) - Operations guide
- [Block Lag Monitor](tools/block-lag-monitor/) - RPC block lag & latency monitoring (any EVM chain + Avalanche)
- [Load Test](tools/load-test/) - JSON-RPC load testing with Paradigm's flood
- [Static Inventory](docs/static-inventory.md) - Deploy without Terraform (manual servers)

## Common Commands

> **Working directories:** Terraform runs from its stack directory (`terraform/stacks/{platform}/{chain}/{network}/`). Ansible runs from the `ansible/` directory. See [Deployment Scenarios](#deployment-scenarios) for complete end-to-end guides.

```bash
# ── Terraform ─────────────────────────────────────────────────────────────────
# Each stack has its own directory - cd into it before running commands.

cd terraform/stacks/aws/ethereum/mainnet    # AWS Mainnet
cd terraform/stacks/aws/ethereum/sepolia    # AWS Sepolia
cd terraform/stacks/latitude/ethereum/mainnet   # Latitude Mainnet
cd terraform/stacks/latitude/ethereum/sepolia   # Latitude Sepolia

terraform init && terraform apply -var-file=nodes.tfvars          # All nodes
terraform apply -var-file=nodes.tfvars -target='module.compute["full"]'  # Specific node
terraform plan -var-file=nodes.tfvars                              # Preview

# ── Ansible ───────────────────────────────────────────────────────────────────
# All ansible commands run from the ansible/ directory.

cd ansible
export TF_TOKEN_app_terraform_io="your-api-token"
export ANSIBLE_PRIVATE_KEY_FILE=~/.ssh/your-key.pem

# Match playbook to node role:
#   role = "ethereum" -> playbooks/ethereum.yml (Docker: Geth+Prysm, Reth+Lighthouse)
#   role = "erigon"   -> playbooks/erigon.yml   (Binary: Erigon+Caplin)

# Deploy all nodes of a role
ansible-playbook -i inventory/ethereum_mainnet_aws_terraform_state.yml playbooks/ethereum.yml
ansible-playbook -i inventory/ethereum_mainnet_aws_terraform_state.yml playbooks/erigon.yml

# Deploy specific node group
ansible-playbook -i inventory/ethereum_mainnet_aws_terraform_state.yml playbooks/erigon.yml --limit node_full
ansible-playbook -i inventory/ethereum_mainnet_aws_terraform_state.yml playbooks/ethereum.yml --limit node_archive

# Mixed roles (Ethereum full + Erigon archive on same inventory)
ansible-playbook -i inventory/ethereum_mainnet_aws_terraform_state.yml playbooks/ethereum.yml --limit node_full
ansible-playbook -i inventory/ethereum_mainnet_aws_terraform_state.yml playbooks/erigon.yml --limit node_archive

# Latitude BM (RAID first, then node playbook)
ansible-playbook -i inventory/ethereum_mainnet_latitude_terraform_state.yml playbooks/raid.yml
ansible-playbook -i inventory/ethereum_mainnet_latitude_terraform_state.yml playbooks/erigon.yml

# HAProxy only (update proxy without redeploying node)
ansible-playbook -i inventory/ethereum_mainnet_aws_terraform_state.yml playbooks/erigon.yml --tags haproxy

# Deploy node without HAProxy
ansible-playbook -i inventory/ethereum_mainnet_aws_terraform_state.yml playbooks/erigon.yml --skip-tags haproxy

# Dry-run (preview changes without applying)
ansible-playbook -i inventory/ethereum_mainnet_aws_terraform_state.yml playbooks/erigon.yml --check --diff

# Static inventory (no Terraform)
ansible-playbook -i inventory/ethereum_sepolia_hosts.yml playbooks/erigon.yml
```

## Security

### AWS (Security Groups)
- SSH access restricted to VPN/office IP ranges only
- Per-client P2P security groups: each node receives only the SGs for its execution and consensus clients (no shared permissive SG)
- IAM instance profile scoped to Route 53 only (Certbot DNS-01 for HAProxy TLS)
- SSH keys managed via AWS key pairs (never commit private keys)
- Optional Elastic IPs for stable endpoints
- IMDSv2 required (no IMDSv1)

### Latitude BM (UFW Firewall)
- UFW enabled on Latitude bare metal nodes via `group_vars/platform_latitude/main.yml` (disabled on AWS - Security Groups handle it)
- VPN IPs get access to Engine API and gRPC via `erigon_firewall_allowed_ips` in group_vars
- P2P, Torrent, Caplin, and DNS ports open to all
- RPC (8545, 8546) bind to `127.0.0.1` only - external access via HAProxy (443)
- Engine API (8551), gRPC (9090) blocked from public - VPN only
- HAProxy ports (80, 443) open to all when `haproxy_firewall_enabled: true`
- SSH (22) open to all (restrict further with `erigon_firewall_allowed_ips` if needed)

### HAProxy (TLS + API Key Auth)
- Let's Encrypt TLS certificates via Certbot + Route 53 DNS-01 (auto-renewal every 90 days)
- UUID4 API key required for all RPC requests (validated via `X-API-Key` header or `?apikey=` URL parameter)
- API keys stored in `/etc/haproxy/api-keys/` (file-based lookup, never inline in config)
- RPC backends bind to `127.0.0.1` (not accessible externally without HAProxy)
- Stats page bound to localhost only by default (port 8404)

## Roadmap

- [x] Ethereum Mainnet archive node (Reth + Lighthouse) - Docker
- [x] Ethereum Mainnet full node (Geth + Prysm) - Docker
- [x] Ethereum Mainnet full node (Erigon + Caplin) - Binary build
- [x] Ethereum Mainnet archive node (Erigon + Caplin) - Binary build
- [x] Ethereum Sepolia full node (Erigon + Caplin) - Binary build
- [x] Ethereum Sepolia archive node (Erigon + Caplin) - Binary build
- [x] Ethereum Sepolia full node (Geth + Prysm) - Docker
- [x] Ethereum Sepolia archive node (Reth + Lighthouse) - Docker
- [x] HAProxy reverse proxy (TLS + API key auth + WebSocket)
- [ ] Monitoring stack (Prometheus, Grafana)
- [ ] Additional chains (Avalanche , BNB , Arbitrum, Optimism, Base , IOTA , Polkadot , Xai , Polygon and more)
- [ ] Multi-cloud support (GCP, Azure)
- [ ] High availability configuration

## License

MIT License - See [LICENSE](LICENSE) for details.
